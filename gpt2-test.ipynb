{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount(\"/content/drive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "% cd \"/content/drive/My Drive/shared-works/tutorials/tpu-colab\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import estimator\n",
    "from gpt2 import GPT2\n",
    "import os\n",
    "import json\n",
    "\n",
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model in eager mode\n",
    "model_path = \"model\"\n",
    "with open(os.path.join(model_path, \"hparams.json\")) as f:\n",
    "    config = json.load(f)\n",
    "model = GPT2(config, name=\"gpt2\")\n",
    "x = tf.zeros([0, 0], dtype=tf.int32)\n",
    "_ = model(x) # build model\n",
    "\n",
    "model.load_weights(os.path.join(model_path, \"weights.h5\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _data_builder(file_path, batch_size, pad_size):\n",
    "    data = tf.data.TextLineDataset(file_path)\n",
    "    data = data.repeat()\n",
    "\n",
    "    def _map(x):\n",
    "        x = tf.expand_dims(x, 0)\n",
    "        tokens = tf.strings.split(x, \" \").values\n",
    "        tokens = tf.strings.to_number(tokens, tf.int32)\n",
    "        length = tf.shape(tokens)[0]\n",
    "        return {\"tokens\": tokens, \"length\": length}\n",
    "\n",
    "    data = data.map(_map)\n",
    "    output_shape = {\"tokens\": tf.TensorShape([pad_size]), \"length\": tf.TensorShape([])}\n",
    "    data = data.padded_batch(batch_size, output_shape)\n",
    "    return data\n",
    "\n",
    "def data_fn():\n",
    "    data_path = \"data\"\n",
    "    train = _data_builder(os.path.join(data_path, \"train.txt\"), 8, 1025)\n",
    "    dev = _data_builder(os.path.join(data_path, \"test.txt\"), 8, 1025)\n",
    "    data_spec = estimator.DataSpec(train=train, dev=dev)\n",
    "    return data_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_fn(data, training):\n",
    "    model = GPT2(config, name=\"gpt2\")\n",
    "    inputs = data[\"tokens\"][:, :-1]\n",
    "    labels = data[\"tokens\"][:, 1:]\n",
    "    dropout = tf.cast(training, tf.float32) * 0.05\n",
    "    logits = model(inputs, use_2d=True, attention_dropout=dropout, dropout=dropout)\n",
    "    labels = tf.reshape(labels, [-1])\n",
    "    loss = tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
    "    mask = tf.sequence_mask(data[\"length\"] - 1, maxlen=labels.shape[1])\n",
    "    mask = tf.reshape(mask, [-1])\n",
    "    mask = tf.cast(mask, loss.dtype)\n",
    "    loss = tf.reduce_sum(mask * loss) / tf.reduce_sum(mask)\n",
    "    lr = tf.Variable(1e-4, name=\"lr\")\n",
    "    model_spec = estimator.ModelSpec(\n",
    "        loss=loss,\n",
    "        optimizer=tf.train.GradientDescentOptimizer(lr),\n",
    "        trainable_variables=model.weights,\n",
    "        import_variables=model.weights\n",
    "    )\n",
    "    return model_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_config = estimator.RunConfig(\n",
    "    train_steps_per_round=200,\n",
    "    eval_steps_per_round=10,\n",
    "    model_dir=\"/content/model\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estm = estimator.Estimator(model_fn, data_fn, run_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = [v.numpy() for v in model.weights]\n",
    "estm.import_variables(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estm.run(2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = estm.export_model()\n",
    "for u, v in zip(values, model.weights):\n",
    "    v.assign(u)\n",
    "model.save_weights(os.path.join(model_path, \"new_weigths.h5\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
